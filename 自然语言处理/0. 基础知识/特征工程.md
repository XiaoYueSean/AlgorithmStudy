>数据和特征决定了机器学习的上限，算法和模型不过是逼近这个上限。

# 文本预处理
对不同的语言，文本预处理方式是不一样的。   
***中文的预处理：***
+ 数据清理（可选）
  + 去停用词
  + 去符号
  + 去除数据中的非中文部分
  + 中文纠错
  + 等等等清理方法(具体还要看使用场景和数据)
+ 分词(必备)

***英文的预处理：***
+ 数据清理（可选）
  + 去停用词 Stop Words
  + 去符号
  + 小写 Capitalization
  + 俚语和缩写(slang and abbreviation) 转化为原词语
  + 拼写纠正 Spelling Correction 
+ 标准化（可选）
  + Stemming
  + Lemmatization
+ 分词（必须）

# 特征提取
xxx

# 文本表示
文本的标志方式有两种，一种是`离散表示`，另一种是`分布式表示`。


## 离散表示
One-hot, BoW, TFIdf, N-gram 都是离散表示。
### 热独(One-Hot)
One-hot表示很容易理解。在一个语料库中，给每个字/词编码一个索引，根据索引进行one-hot表示。One-hot 只是对`词`的一种编码方式！！！
> Sean likes to watch movies. 
> Sean is a NLPer.

如果只需要表示出上面一句话中的单词，可以只对其中出现过的单词进行索引编码：

> {"Sean": 1, "likes": 2, "to": 3, "watch": 4, "movies": 5, "is" : 6, "a" : 7, "NLPer": 8}

其中每个单词都可以用one-hot方法表示：
> Sean: [1, 0, 0, 0, 0, 0, 0, 0]  
> likes: [0, 1, 0, 0, 0, 0, 0, 0]

One-Hot的缺陷： 
+ 数据稀疏和维度灾难。数据稀疏也就是向量的大部分元素为0，如果词袋中的字词达数百万个，那么由每篇文档转换成的向量的维度是数百万维.
+ 没有考虑句中字的顺序性，假定字之间相互独立。这意味着意思不同的句子可能得到一样的向量。 "我爱你" "你爱我"。 这两句话的one-hot编码都是一样，但意思是不同但。
+ 没有考虑字但相对重要性。
### 词袋（Bag-of-Word）
词袋表示，也称为计数向量表示(Count Vectors)。文档的向量表示可以直接用单词的向量进行求和得到。等于说BoW把句子中的每个词的向量相加，得到的特征向量。
> Sean likes to watch movies. -->> [1, 1, 1, 1, 1, 0, 0, 0]  
> Sean is a NLPer. Sean Sean -->> [3, 0, 0, 0, 0, 0, 1, 1]  


### N-gram
N-Gram是一种基于统计语言模型的算法。它的基本思想是将文本里面的内容按照字节进行大小为N的滑动窗口操作，形成了长度是N的字节片段序列。

## 分布式表示
